{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(f):\n",
    "\n",
    "    def timed(*args, **kw):\n",
    "\n",
    "        ts = time.time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time.time()\n",
    "\n",
    "        print('func:%r took: %2.4f sec' % (f.__name__,  te-ts))\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent\n",
    "\n",
    "Solve the following optimizations problem for the function\n",
    "\n",
    "$$\n",
    "ð‘“(ð‘¥, ð‘¦) = ð‘¥4\n",
    "âˆ’ ð‘¥2 + ð‘¦2 + 2ð‘¥ð‘¦ âˆ’ 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function3d(point):\n",
    "    \"\"\"\n",
    "    This function calculates the value of a function of x and y  at a certain point\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y: the point to evaluate the function at\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "     the function value at the point\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "\n",
    "    return x**4 - x**2 + y**2 + 2*x*y - 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_derivative(point):\n",
    "    \"\"\"\n",
    "    This function calculates the first derivative of a function at a given point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    point: list. Not really a list, more like comma separated variables\n",
    "    A point, x,y\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    first_derivative: np.array\n",
    "    The first derivative (gradient) at the point entered\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "    dfdx = (4 * x**3) - (2*x) + 2*y\n",
    "    dfdy = (2*y) + 2*x\n",
    "\n",
    "    return np.array([dfdx, dfdy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "\n",
    "def steepest_descent_one_step(func, gradient, x0, alpha=0.1):\n",
    "    \"\"\"\n",
    "    This function performs a single step of the steepest descent algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    func:\n",
    "    function to optimize\n",
    "\n",
    "    gradient: np.array\n",
    "    gradient of a function\n",
    "\n",
    "    x0: np.array\n",
    "    starting point\n",
    "\n",
    "    num_iter: int\n",
    "    number of times the for loop will run\n",
    "\n",
    "    alpha: float\n",
    "    stepsize\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x1 : new position\n",
    "    value: funciton evaluated at that x1\n",
    "    \"\"\"\n",
    "    point_searched = []\n",
    "    # calculate the initial function value\n",
    "    prev_value = func(x0)\n",
    "\n",
    "    # print search progress and keep track of the points searched\n",
    "    print(f\"searching at {x0} with function value {prev_value}\")\n",
    "    point_searched.append(x0)\n",
    "        \n",
    "    # calculate the gradient at the current point\n",
    "    g = gradient(x0)\n",
    "\n",
    "    # calculate next point and its function value\n",
    "    x1 = x0 - g * alpha\n",
    "    #value = func(x1)\n",
    "\n",
    "    return print(f\"New position is {x1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching at [1.5 1.5] with function value 7.5625\n",
      "New position is [0.15 0.9 ]\n",
      "func:'steepest_descent_one_step' took: 0.0006 sec\n"
     ]
    }
   ],
   "source": [
    "steepest_descent_one_step(function3d, first_derivative, np.array([1.5, 1.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good optimization step because it moves towards the minimum. We will increase the stepsize * 1.2 in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import numpy.linalg as LA\n",
    "\n",
    "@timeit\n",
    "def steepest_descent(func,func_gradient,x0, alpha,tol):\n",
    "    \"\"\"\n",
    "    This function finds a local minimum using the steepest descent method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    func:\n",
    "    The function whose minima we plan to find (inputted as a function)\n",
    "    \n",
    "    func_gradient:\n",
    "    first_derivative of func\n",
    "\n",
    "    x0: np.array\n",
    "    position from which we start searching for the minima\n",
    "\n",
    "    alpha: float\n",
    "    step size\n",
    "\n",
    "    tol: float\n",
    "    tolerance value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Starting point: entered x0\n",
    "    Evaluation: value of function at x0\n",
    "    Path to minimum: lists all points evaluated on the way to minumum\n",
    "    Steps to converge: counts number of steps until local mimimum is reached.\n",
    "    \"\"\"\n",
    "    count= 0\n",
    "    visited =[x0]\n",
    "    deriv = func_gradient(x0)\n",
    "    \n",
    "    while LA.norm(deriv) > tol and count < 1e6:\n",
    "        # calculate new point position\n",
    "        x1 = x0 - deriv * alpha\n",
    "        \n",
    "        if func(x1) < func(x0):\n",
    "            # Check if new value is less than previous value. If so, this is a good move and we can increase step size\n",
    "            # and start the next search step from the current value\n",
    "            alpha = alpha * 1.2\n",
    "            x0 = x1\n",
    "            deriv = func_gradient(x0)\n",
    "            visited.append(x1)\n",
    "            \n",
    "        else:\n",
    "            # If new_value > prev_value, we are moving in the wrong direction. So we decrease step size\n",
    "            # and redo that step starting from the previous value.\n",
    "            alpha = alpha * 0.5\n",
    "            #visited.append(x0)\n",
    "\n",
    "        count+=1\n",
    "    return {\"Starting point\":x0,\"Evaluation\":func(x0),\"Path to minimum\":np.asarray(visited), \"Steps to converge\":len(visited)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'steepest_descent' took: 0.0005 sec\n",
      "{'Starting point': array([-1.00000072,  1.0000014 ]), 'Evaluation': -2.999999999997453, 'Path to minimum': array([[-1.5       ,  1.5       ],\n",
      "       [-0.75      ,  1.5       ],\n",
      "       [-1.0875    ,  1.32      ],\n",
      "       [-1.04004413,  1.25304   ],\n",
      "       [-1.05492903,  1.17942863],\n",
      "       [-1.00779561,  1.12779615],\n",
      "       [-1.05181525,  1.0680762 ],\n",
      "       [-0.98988976,  1.06322071],\n",
      "       [-1.03043727,  1.03694491],\n",
      "       [-1.00445425,  1.03554582],\n",
      "       [-1.00784819,  1.02752454],\n",
      "       [-1.00410623,  1.021433  ],\n",
      "       [-1.00440363,  1.01499603],\n",
      "       [-1.00122119,  1.01027389],\n",
      "       [-1.00344611,  1.005431  ],\n",
      "       [-0.99963579,  1.00479389],\n",
      "       [-1.00218339,  1.00280712],\n",
      "       [-1.00030255,  1.00266297],\n",
      "       [-1.00062139,  1.00200836],\n",
      "       [-1.00025502,  1.00154679],\n",
      "       [-1.00036336,  1.00103092],\n",
      "       [-0.99998637,  1.00071101],\n",
      "       [-1.0002104 ,  1.00050266],\n",
      "       [-1.00002076,  1.00040182],\n",
      "       [-1.00014415,  1.00024404],\n",
      "       [-1.00002569,  1.00021923],\n",
      "       [-1.00005275,  1.00016153],\n",
      "       [-1.00001618,  1.00012262],\n",
      "       [-1.00003409,  1.00007692],\n",
      "       [-0.99998591,  1.00005486],\n",
      "       [-1.00002464,  1.00003355],\n",
      "       [-0.99999138,  1.00003024],\n",
      "       [-1.0000077 ,  1.0000216 ],\n",
      "       [-1.00000319,  1.00001789],\n",
      "       [-1.00000381,  1.00001317],\n",
      "       [-1.00000155,  1.00000957],\n",
      "       [-1.00000239,  1.00000587],\n",
      "       [-0.99999901,  1.00000395],\n",
      "       [-1.00000196,  1.00000231],\n",
      "       [-0.99999897,  1.00000217],\n",
      "       [-1.00000072,  1.0000014 ]]), 'Steps to converge': 41}\n"
     ]
    }
   ],
   "source": [
    "print(steepest_descent(function3d, first_derivative, np.array([-1.5, 1.5]), 0.1, 1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function took 41 good steps to converge. The total number of steps taken was 51."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -3.000000\n",
      "         Iterations: 6\n",
      "         Function evaluations: 39\n",
      "         Gradient evaluations: 13\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: -2.9999999999997273\n",
      "       x: [-1.000e+00  1.000e+00]\n",
      "     nit: 6\n",
      "     jac: [ 2.414e-06  5.364e-07]\n",
      "    nfev: 39\n",
      "    njev: 13\n"
     ]
    }
   ],
   "source": [
    "# Conjugate gradient\n",
    "x0 = np.array([-1.5, 1.5])\n",
    "\n",
    "CG_method = minimize(function3d, x0, method='CG', options={'gtol': 1e-5, 'disp': True})\n",
    "print(CG_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -3.000000\n",
      "         Iterations: 7\n",
      "         Function evaluations: 27\n",
      "         Gradient evaluations: 9\n",
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: -2.999999999999986\n",
      "        x: [-1.000e+00  1.000e+00]\n",
      "      nit: 7\n",
      "      jac: [ 4.172e-07  2.384e-07]\n",
      " hess_inv: [[ 1.244e-01 -1.270e-01]\n",
      "            [-1.270e-01  6.174e-01]]\n",
      "     nfev: 27\n",
      "     njev: 9\n"
     ]
    }
   ],
   "source": [
    "# BFGS\n",
    "x0 = np.array([-1.5, 1.5])\n",
    "\n",
    "BFGS_method = minimize(function3d, x0, method='BFGS', options={'gtol': 1e-5, 'disp': True})\n",
    "print(BFGS_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of number of steps, both conjugate gradient (CG) and BFGS are more efficient than steepest descent. CG is only a bit more efficident (39 steps instead of 41) whie BFGS takes only 27 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Local optimization and machine learning using Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_function3d(point):\n",
    "    \"\"\"\n",
    "    This function calculates the value of a function of x and y  at a certain point\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y: the point to evaluate the function at\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "     the function value at the point\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "\n",
    "    return (1 - x)**2 + 10* (y- (x**2))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_gradient(point):\n",
    "    \"\"\"\n",
    "    This function calculates the first derivative of a function at a given point.\n",
    "    Parameters\n",
    "    ----------\n",
    "    point: list. Not really a list, more like comma separated variables\n",
    "    A point, x,y\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    first_derivative: np.array\n",
    "    The first derivative (gradient) at the point entered\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "    dfdx = -2 *(1-x) - 40 * x *(y-x**2)\n",
    "    dfdy = 20 * (y-x**2)\n",
    "\n",
    "    return np.array([dfdx, dfdy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'steepest_descent' took: 0.0093 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Starting point': array([0.99999089, 0.99998153]),\n",
       " 'Evaluation': 8.361266796946337e-11,\n",
       " 'Path to minimum': array([[-0.5       ,  1.5       ],\n",
       "        [-1.05      ,  0.875     ],\n",
       "        [-0.845175  ,  0.94325   ],\n",
       "        ...,\n",
       "        [ 0.99999068,  0.99998135],\n",
       "        [ 0.99999093,  0.99998135],\n",
       "        [ 0.99999089,  0.99998153]]),\n",
       " 'Steps to converge': 1205}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steepest_descent(rosenbrock_function3d, rosenbrock_gradient,np.array([-0.5, 1.5]), 0.1, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "\n",
    "def stochastic_gradient_descent(func,func_gradient,x0,alpha=0.1,tol=1e-5,stochastic_injection=1):\n",
    "    '''\n",
    "    This function finds a local minimum using the stochastic gradient method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    func:\n",
    "    The function whose minima we plan to find (inputted as a function)\n",
    "    \n",
    "    func_gradient:\n",
    "    first_derivative of func\n",
    "\n",
    "    x0: np.array\n",
    "    position from which we start searching for the minima\n",
    "\n",
    "    alpha: float\n",
    "    step size\n",
    "\n",
    "    tol: float\n",
    "    tolerance value\n",
    "\n",
    "    stochastic_injection: 0 or 1\n",
    "    controls the magnitude of stochasticity (multiplied with stochastic_deriv)\n",
    "    0 for no stochasticity, equivalent to SD. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Starting point: entered x0\n",
    "    Evaluation: value of function at x0\n",
    "    Path to minimum: lists all points evaluated on the way to minumum\n",
    "    Steps to converge: counts number of steps until local mimimum is reached.\n",
    "    '''\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv = func_gradient(x0)\n",
    "\n",
    "    count=0\n",
    "    visited=[x0]\n",
    "    while LA.norm(deriv) > tol and count < 1e5:\n",
    "        if stochastic_injection>0:\n",
    "            # formulate a stochastic_deriv that is the same norm as your gradient \n",
    "            #dim = deriv.shape\n",
    "            stochastic_deriv= np.random.random(2) * 2 - 1\n",
    "            stochastic_norm = LA.norm(stochastic_deriv)\n",
    "            stochastic_deriv = stochastic_deriv / stochastic_norm * LA.norm(deriv)\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(x0))\n",
    "        direction=-(deriv + stochastic_injection * stochastic_deriv)\n",
    "        # calculate new point position\n",
    "        x1 = x0 - deriv * alpha\n",
    "        \n",
    "        if func(x1) < func(x0):\n",
    "            # Check if new value is less than previous value. If so, this is a good move and we can increase step size\n",
    "            # and start the next search step from the current value\n",
    "            alpha = alpha * 1.2\n",
    "            x0 = x1\n",
    "            visited.append(x1)\n",
    "            deriv = func_gradient(x1)\n",
    "            #print(f'good step {x1}')\n",
    "            \n",
    "        else:    \n",
    "            # If new_value > prev_value, we are moving in the wrong direction. So we decrease step size\n",
    "            # and redo that step starting from the previous value.\n",
    "            alpha = alpha * 0.5\n",
    "            #print(f'bad step {x1}')\n",
    "            \n",
    "        count+=1\n",
    "    return {\"x\":x0,\"evaluation\":func(x0),\"path\":np.asarray(visited), \"Number of steps\": len(visited)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'stochastic_gradient_descent' took: 0.0183 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': array([0.99999089, 0.99998153]),\n",
       " 'evaluation': 8.361266796946337e-11,\n",
       " 'path': array([[-0.5       ,  1.5       ],\n",
       "        [-1.05      ,  0.875     ],\n",
       "        [-0.845175  ,  0.94325   ],\n",
       "        ...,\n",
       "        [ 0.99999068,  0.99998135],\n",
       "        [ 0.99999093,  0.99998135],\n",
       "        [ 0.99999089,  0.99998153]]),\n",
       " 'Number of steps': 1205}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochastic_gradient_descent(rosenbrock_function3d, rosenbrock_gradient, np.array([-0.5, 1.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 20\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 44\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 2.0711221375743512e-13\n",
      "       x: [ 1.000e+00  1.000e+00]\n",
      "     nit: 20\n",
      "     jac: [ 4.992e-08 -2.474e-08]\n",
      "    nfev: 132\n",
      "    njev: 44\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-0.5, 1.5])\n",
    "\n",
    "CG_rosenbrock = minimize(rosenbrock_function3d, x0, method='CG', options={'gtol': 1e-5, 'disp': True})\n",
    "print(CG_rosenbrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 22\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 31\n",
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 1.6857105436734322e-13\n",
      "        x: [ 1.000e+00  1.000e+00]\n",
      "      nit: 22\n",
      "      jac: [ 1.153e-07 -1.294e-08]\n",
      " hess_inv: [[ 5.099e-01  1.020e+00]\n",
      "            [ 1.020e+00  2.089e+00]]\n",
      "     nfev: 93\n",
      "     njev: 31\n"
     ]
    }
   ],
   "source": [
    "BFGS_rosenbrock = minimize(rosenbrock_function3d, x0, method='BFGS', options={'gtol': 1e-5, 'disp': True})\n",
    "print(BFGS_rosenbrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the Rosenbrock Banana Function, CG and BFGS are far better than SGD. SGD found the minimum in ~1200 steps, while CG and BFGS took 132 and 93, respectively.\n",
    "\n",
    "### 3d\n",
    "No. Because of the stochasticity, number of steps with SGD will vary largely and so you need to take an average value after a few runs for comparison. \n",
    "\n",
    "### 3e\n",
    "I found the performance of SGD to be more consistent than that of steepest descent. For example, at point (-1.5, 1.5), SGD and steepest descent took ~1200 steps to converge. However, as the numbers got bigger the difference in step size grew. At point (-150, 150), SGD took ~79000 steps to converge while steepest descent took ~490000. At (-1500, 1500), SGD still took ~79000 steps while steepest descent took ~790000. \n",
    "\n",
    "However when comparing CD and BFGS with SGD, their performances (in terms of step sizes) did not increase as quickly as the step size of steepest descent did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Stochastic Gradient Descent with Momentum (SGDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_function3d(point):\n",
    "    \"\"\"\n",
    "    This function calculates the value of a function of x and y  at a certain point\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y: the point to evaluate the function at\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "     the function value at the point\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "\n",
    "    return 2*x**2 - 1.05*x**4 + x**6/6 + x*y + y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_gradient(point):\n",
    "    \"\"\"\n",
    "    This function calculates the first derivative of a function at a given point.\n",
    "    Parameters\n",
    "    ----------\n",
    "    point: list. Not really a list, more like comma separated variables\n",
    "    A point, x,y\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    first_derivative: np.array\n",
    "    The first derivative (gradient) at the point entered\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "    dfdx = 4*x - 4.2*x**3 + x**5 + y\n",
    "    dfdy = x + 2*y \n",
    "\n",
    "    return np.array([dfdx, dfdy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'stochastic_gradient_descent' took: 0.0020 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': array([-1.74755328,  0.87377865]),\n",
       " 'evaluation': 0.29863844224600855,\n",
       " 'path': array([[-1.5       ,  1.5       ],\n",
       "        [-1.708125  ,  1.35      ],\n",
       "        [-1.81711465,  1.230975  ],\n",
       "        [-1.72366291,  1.13811871],\n",
       "        [-1.81648029,  1.04263383],\n",
       "        [-1.73077839,  1.01476596],\n",
       "        [-1.77260058,  0.97759624],\n",
       "        [-1.73965321,  0.95033542],\n",
       "        [-1.77022043,  0.92148765],\n",
       "        [-1.74397071,  0.91366684],\n",
       "        [-1.75467953,  0.90291347],\n",
       "        [-1.74553402,  0.89499619],\n",
       "        [-1.7540083 ,  0.88673796],\n",
       "        [-1.74656205,  0.88456827],\n",
       "        [-1.74961089,  0.88154912],\n",
       "        [-1.74682193,  0.87938454],\n",
       "        [-1.74960838,  0.87708366],\n",
       "        [-1.74708789,  0.87655687],\n",
       "        [-1.74825522,  0.8757213 ],\n",
       "        [-1.74715493,  0.87519094],\n",
       "        [-1.74777788,  0.87486877],\n",
       "        [-1.74758021,  0.87463399],\n",
       "        [-1.74765485,  0.87439135],\n",
       "        [-1.74754601,  0.87419677],\n",
       "        [-1.74764902,  0.87402131],\n",
       "        [-1.74753349,  0.87397242],\n",
       "        [-1.74759689,  0.87391111],\n",
       "        [-1.74752418,  0.8738708 ],\n",
       "        [-1.74757107,  0.87384747],\n",
       "        [-1.74755094,  0.87383152],\n",
       "        [-1.74756213,  0.87381419],\n",
       "        [-1.74754713,  0.87380191],\n",
       "        [-1.74755705,  0.8737956 ],\n",
       "        [-1.74755201,  0.87379104],\n",
       "        [-1.74755504,  0.87378622],\n",
       "        [-1.74755067,  0.87378288],\n",
       "        [-1.74755379,  0.87378114],\n",
       "        [-1.74755205,  0.87377996],\n",
       "        [-1.74755328,  0.87377865]]),\n",
       " 'Number of steps': 39}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochastic_gradient_descent(momentum_function3d, momentum_gradient, np.array([-1.5, 1.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 7\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 21\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 0.29863844223965763\n",
      "       x: [-1.748e+00  8.738e-01]\n",
      "     nit: 7\n",
      "     jac: [ 8.404e-06  7.227e-07]\n",
      "    nfev: 63\n",
      "    njev: 21\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.5, -1.5])\n",
    "\n",
    "CG_momentum = minimize(momentum_function3d, x0, method='CG', options={'gtol': 1e-5, 'disp': True})\n",
    "print(CG_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 8\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 10\n",
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 0.29863844223686065\n",
      "        x: [-1.748e+00  8.738e-01]\n",
      "      nit: 8\n",
      "      jac: [ 1.341e-07 -7.451e-09]\n",
      " hess_inv: [[ 8.569e-02 -4.290e-02]\n",
      "            [-4.290e-02  5.109e-01]]\n",
      "     nfev: 30\n",
      "     njev: 10\n"
     ]
    }
   ],
   "source": [
    "BFGS_momentum = minimize(momentum_function3d, x0, method='BFGS', options={'gtol': 1e-5, 'disp': True})\n",
    "print(BFGS_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, stochastic gradient descent did better than conjugate gradients (~40 steps VS ~60 steps). Stochastic gradient and BFGS had roughly the same performance (~40 steps and ~30 steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "\n",
    "def SGDM(func,func_gradient,x0,alpha=0.1,gamma=0.9,tol=1e-5,stochastic_injection=1):\n",
    "    '''\n",
    "    This function finds a local minimum using the stochastic gradient method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    func:\n",
    "    The function whose minima we plan to find (inputted as a function)\n",
    "    \n",
    "    func_gradient:\n",
    "    first_derivative of func\n",
    "\n",
    "    x0: np.array\n",
    "    position from which we start searching for the minima\n",
    "\n",
    "    alpha: float\n",
    "    step size\n",
    "\n",
    "    gamma: float\n",
    "    momentum value\n",
    "\n",
    "    tol: float\n",
    "    tolerance value\n",
    "\n",
    "    stochastic_injection: 0 or 1\n",
    "    controls the magnitude of stochasticity (multiplied with stochastic_deriv)\n",
    "    0 for no stochasticity, equivalent to SD. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Starting point: entered x0\n",
    "    Evaluation: value of function at x0\n",
    "    Path to minimum: lists all points evaluated on the way to minumum\n",
    "    Steps to converge: counts number of steps until local mimimum is reached.\n",
    "    '''\n",
    "    # evaluate the gradient at starting point\n",
    "    deriv = func_gradient(x0)\n",
    "    \n",
    "    count=0\n",
    "    visited=[x0]\n",
    "    while LA.norm(deriv) > tol and count < 1e5:\n",
    "        if stochastic_injection>0:\n",
    "            # formulate a stochastic_deriv that is the same norm as your gradient \n",
    "            stochastic_deriv= np.random.random(2) * 2 - 1\n",
    "            stochastic_norm = LA.norm(stochastic_deriv)\n",
    "            stochastic_deriv = stochastic_deriv / stochastic_norm * LA.norm(deriv)\n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(x0))\n",
    "        if count == 0:\n",
    "            previous_direction = -deriv\n",
    "        direction=-(deriv+stochastic_injection*stochastic_deriv + gamma * previous_direction)\n",
    "        x1 = x0 + alpha * direction\n",
    "        \n",
    "        if func(x1) < func(x0):\n",
    "            # Check if new value is less than previous value. If so, this is a good move and we can increase step size\n",
    "            # and start the next search step from the current value\n",
    "            alpha = alpha * 1.2\n",
    "            x0 = x1\n",
    "            visited.append(x1)\n",
    "            deriv = func_gradient(x0)\n",
    "            \n",
    "        else:\n",
    "            # If new_value > prev_value, we are moving in the wrong direction. So we decrease step size\n",
    "            # and redo that step starting from the previous value.\n",
    "            if alpha<1e-5:\n",
    "                previous_direction=previous_direction-previous_direction\n",
    "            else:\n",
    "                # do the same as SGD here\n",
    "                alpha = alpha * 0.5\n",
    "\n",
    "        count+=1\n",
    "    return {\"x\":x0,\"Evaluation\":func(x0),\"Path\":np.asarray(visited), \"Number of step to converge\": len(visited)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'SGDM' took: 0.0030 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': array([-1.74755242,  0.87377317]),\n",
       " 'Evaluation': 0.2986384422461035,\n",
       " 'Path': array([[-1.5       , -1.5       ],\n",
       "        [-1.42887291, -0.99991264],\n",
       "        [-1.52341299, -0.95558559],\n",
       "        [-1.51358025, -0.93314037],\n",
       "        [-1.50819735, -0.90514635],\n",
       "        [-1.51171835, -0.87149957],\n",
       "        [-1.51193333, -0.8702979 ],\n",
       "        [-1.5138259 , -0.86970886],\n",
       "        [-1.51209007, -0.86936447],\n",
       "        [-1.51013788, -0.86869735],\n",
       "        [-1.50914783, -0.86659327],\n",
       "        [-1.50780145, -0.86603561],\n",
       "        [-1.50810051, -0.86426464],\n",
       "        [-1.50881599, -0.86328878],\n",
       "        [-1.50874209, -0.86205446],\n",
       "        [-1.50871703, -0.86196794],\n",
       "        [-1.50859369, -0.86197152],\n",
       "        [-1.50861021, -0.86193535],\n",
       "        [-1.50862084, -0.86188384],\n",
       "        [-1.50860418, -0.86182104],\n",
       "        [-1.50856939, -0.86180588],\n",
       "        [-1.50857772, -0.86180438],\n",
       "        [-1.50862888, -0.86175653],\n",
       "        [-1.50857728, -0.86163747],\n",
       "        [-1.50853016, -0.8614841 ],\n",
       "        [-1.50859024, -0.86131583],\n",
       "        [-1.5086977 , -0.86120214],\n",
       "        [-1.50868289, -0.86091939],\n",
       "        [-1.50877758, -0.8606204 ],\n",
       "        [-1.50864942, -0.86058901],\n",
       "        [-1.50865986, -0.86010272],\n",
       "        [-1.5083494 , -0.85988323],\n",
       "        [-1.50825608, -0.85918581],\n",
       "        [-1.50857053, -0.85853138],\n",
       "        [-1.50877006, -0.85847055],\n",
       "        [-1.50849573, -0.85843156],\n",
       "        [-1.50899962, -0.85724743],\n",
       "        [-1.50935077, -0.85561289],\n",
       "        [-1.51029808, -0.8547077 ],\n",
       "        [-1.51118549, -0.85268783],\n",
       "        [-1.50958643, -0.85082981],\n",
       "        [-1.50861662, -0.8506436 ],\n",
       "        [-1.50822936, -0.85064252],\n",
       "        [-1.51060204, -0.84818799],\n",
       "        [-1.513403  , -0.84562414],\n",
       "        [-1.50988777, -0.83994568],\n",
       "        [-1.50739991, -0.8315602 ],\n",
       "        [-1.51145361, -0.82912219],\n",
       "        [-1.51563708, -0.82707445],\n",
       "        [-1.51740927, -0.82671246],\n",
       "        [-1.52416621, -0.81224243],\n",
       "        [-1.52789236, -0.79137261],\n",
       "        [-1.5139853 , -0.77658175],\n",
       "        [-1.50102341, -0.7515267 ],\n",
       "        [-1.50348395, -0.75121575],\n",
       "        [-1.51786004, -0.74459859],\n",
       "        [-1.54127613, -0.72579468],\n",
       "        [-1.52719251, -0.66603555],\n",
       "        [-1.51817571, -0.66527007],\n",
       "        [-1.53778436, -0.58585119],\n",
       "        [-1.56126312, -0.49547162],\n",
       "        [-1.52436188, -0.39937429],\n",
       "        [-1.47632375, -0.36966609],\n",
       "        [-1.50214622, -0.23476685],\n",
       "        [-1.44060314, -0.16033728],\n",
       "        [-1.4122792 , -0.14722699],\n",
       "        [-1.37314195,  0.0151895 ],\n",
       "        [-1.4798359 ,  0.15702739],\n",
       "        [-1.6047083 ,  0.33006197],\n",
       "        [-1.73508535,  0.49289655],\n",
       "        [-1.77033475,  0.63790684],\n",
       "        [-1.64515048,  0.78308844],\n",
       "        [-1.76231657,  0.83695996],\n",
       "        [-1.74609625,  0.86784473],\n",
       "        [-1.74631626,  0.87044695],\n",
       "        [-1.7476742 ,  0.87151245],\n",
       "        [-1.74729029,  0.87140865],\n",
       "        [-1.74700886,  0.87212268],\n",
       "        [-1.74809577,  0.87290828],\n",
       "        [-1.74702504,  0.87323309],\n",
       "        [-1.74733609,  0.87281383],\n",
       "        [-1.74728185,  0.87308691],\n",
       "        [-1.7474133 ,  0.87351685],\n",
       "        [-1.74759802,  0.87362149],\n",
       "        [-1.74753497,  0.87372373],\n",
       "        [-1.74754153,  0.87373789],\n",
       "        [-1.74754202,  0.87374692],\n",
       "        [-1.74754606,  0.87374306],\n",
       "        [-1.74754335,  0.87374985],\n",
       "        [-1.74755902,  0.8737628 ],\n",
       "        [-1.74755931,  0.87377075],\n",
       "        [-1.74754841,  0.87376586],\n",
       "        [-1.74755381,  0.87377095],\n",
       "        [-1.74755244,  0.87377015],\n",
       "        [-1.74755277,  0.87377151],\n",
       "        [-1.74755084,  0.87377256],\n",
       "        [-1.74755109,  0.87377346],\n",
       "        [-1.74755242,  0.87377317]]),\n",
       " 'Number of step to converge': 98}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGDM(momentum_function3d, momentum_gradient, np.array([-1.5, -1.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b\n",
    "No, I did not get a better result when using SGDM. When comparing number of steps, SGD took the fewest number of steps to find global minimum. This was unexpected, as I thought that SGD with momentum would perform better. Since the momentum takes previous good steps into consideration, I thought momentum would serve as a guiding force to move the search in the right direction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
