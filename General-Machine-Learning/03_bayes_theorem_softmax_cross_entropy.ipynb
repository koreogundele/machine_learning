{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as Adam\n",
    "from pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baye's Theorem\n",
    "\n",
    "M is a marker that determines genetic disposition to kidney disease. A chemical test can show if you are positive or negative for M. However, test is not 100% right.\n",
    "y = +ve or -ve\n",
    "x = marker (M) or no marker (no M)\n",
    "$$\n",
    "P[ +| M] = 0.95\n",
    "$$\n",
    "$$\n",
    "P[- | no M] = 0.95\n",
    "$$\n",
    "$$\n",
    "P[M] = 0.01\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a.\n",
    "Define the following quantities:\n",
    "$$ P[-|M]; $$\n",
    "$$ P[+|not M]; $$\n",
    "$$ P[not M] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P[-|M] = 0.05\n",
    "\n",
    "P[+|not M] = 0.05\n",
    "\n",
    "P[not M] = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b.\n",
    "You have had a chemical test and have tested positive; should you be alarmed? To answer this, find what is the chance that a randomly selected person who tests positive for the marker actually has the marker by using Baye’s Theorem. What feature of the given data accounts for the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The odds that Korede actually has M are 0.1667.\n"
     ]
    }
   ],
   "source": [
    "have_M = 0.01\n",
    "have_M_test_pos = 0.95\n",
    "\n",
    "no_M = 0.95\n",
    "no_M_test_pos = 0.05\n",
    "\n",
    "actually_have_M = have_M * have_M_test_pos\n",
    "false_positive = no_M * no_M_test_pos\n",
    "\n",
    "all_positive_tests = actually_have_M + false_positive\n",
    "odds_that_Korede_has_M = actually_have_M / all_positive_tests\n",
    "\n",
    "print(f'The odds that Korede actually has M are {round(odds_that_Korede_has_M, 4)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Baye's Theorem, I would not be too worried about actually having M. The feature of the data that accounts for this result is the occurence of M in the population (0.01)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c.\n",
    "Suppose that frequency of marker was higher by a factor of 10, i.e. $$P[M] = 0.10$$ What is the chance that a randomly selected individual from this group who test positive actually has the marker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Scenario:\n",
    "\n",
    "P[M] = 0.10\n",
    "\n",
    "P[not M] = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The odds that Korede actually has M are 0.6786!\n"
     ]
    }
   ],
   "source": [
    "have_M = 0.1\n",
    "have_M_test_pos = 0.95\n",
    "\n",
    "no_M = 0.90\n",
    "no_M_test_pos = 0.05\n",
    "\n",
    "actually_have_M = have_M * have_M_test_pos\n",
    "false_positive = no_M * no_M_test_pos\n",
    "\n",
    "all_positive_tests = actually_have_M + false_positive\n",
    "odds_that_Korede_has_M = actually_have_M / all_positive_tests\n",
    "\n",
    "print(f'The odds that Korede actually has M are {round(odds_that_Korede_has_M, 4)}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omg my odds are so much higher now. I would be worried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gaussian Naive Bayes\n",
    "We will cluster 178 wines into 3 cultivars by solving with Naïve Bayes. To do this we will classify the wines by assigning them to the cultivar with the largest $$P(cultivar | X)$$ and to find this we must first define a labelled data set of $$P(wine  attribute  x | cultivar)$$ pairings to learn the relationship where x is one of the attributes, and do this for all attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.51861254, -0.5622498 ,  0.23205254, ...,  0.36217728,\n",
       "         1.84791957,  1.01300893],\n",
       "       [ 0.29570023,  0.22769377,  1.84040254, ...,  0.36217728,\n",
       "         0.44960118, -0.03787401],\n",
       "       [ 2.25977152, -0.62508622, -0.7183361 , ...,  0.53767082,\n",
       "         0.33660575,  0.94931905],\n",
       "       ...,\n",
       "       [ 0.20923168,  0.22769377,  0.01273209, ..., -1.56825176,\n",
       "        -1.40069891,  0.29649784],\n",
       "       [ 1.39508604,  1.58316512,  1.36520822, ..., -1.52437837,\n",
       "        -1.42894777, -0.59516041],\n",
       "       [-0.92721209, -0.54429654, -0.90110314, ...,  0.18668373,\n",
       "         0.78858745, -0.7543851 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wines = pd.read_csv('data/wines.csv')\n",
    "\n",
    "x = wines.drop([\"Start assignment\",\"ranking\"],axis=1).values\n",
    "y = wines['ranking'].values\n",
    "scaler = StandardScaler()\n",
    "x_norm = scaler.fit_transform(x)\n",
    "x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a.\n",
    "How should we represent $$ P(wine attribute x | cultivar) $$ Given a wine that belongs to cultivar 1, what is the chance of it having an Alcohol % of 13 according to the probability distribution function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    def __init__(self):\n",
    "        self.type_indices = {}    # store the indices of wines that belong to each cultivar as a boolean array\n",
    "        self.type_stats = {}      # store the mean and std of each cultivar\n",
    "        self.ndata = 0\n",
    "        self.trained = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def gaussian(x, mean, std):\n",
    "        return 1 / (np.sqrt(2 * np.pi) * std) * np.exp(-(x - mean) **2 / (2 * std * 2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_statistics(x_values):\n",
    "        # Returns a list with length of input features. Each element is a tuple, with the input feature's average and standard deviation\n",
    "        n_feats = x_values.shape[1]\n",
    "        return [(np.average(x_values[:, n]), np.std(x_values[:, n])) for n in range(n_feats)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_prob(x_input, stats):\n",
    "        \"\"\"Calculate the probability that the input features belong to a specific class(P(X|C)), defined by the statistics of features in that class\n",
    "        x_input: np.array shape(nfeatures)\n",
    "        stats: list of tuple [(mean1, std1), (means2,std2),...]\n",
    "        \"\"\" \n",
    "        init_prob = 1.0\n",
    "        # P(X|C) = P(x1|C)*P(x2|C)*... where each of them is a gaussian\n",
    "        for x_value, single_stat in zip(x_input, stats):\n",
    "            init_prob *= NaiveBayesClassifier.gaussian(x_value, *single_stat)\n",
    "        return init_prob\n",
    "    \n",
    "    def fit(self,xs,ys):\n",
    "        # Train the classifier by calculating the statistics of different features in each class\n",
    "        self.ndata = len(ys)\n",
    "        for y in set(ys):\n",
    "            type_filter = (ys == y)\n",
    "            self.type_indices[y] = type_filter\n",
    "            self.type_stats[y] = self.calculate_statistics(xs[type_filter])\n",
    "        self.trained = True\n",
    "            \n",
    "    def predict(self,xs):\n",
    "        # Do the prediction by outputing the class that has highest probability\n",
    "        if len(xs.shape) > 1:\n",
    "            print(\"Only accepts one sample at a time!\")\n",
    "        if self.trained:\n",
    "            guess = None\n",
    "            max_prob = 0\n",
    "            # P(C|X) = P(X|C)*P(C) / sum_i(P(X|C_i)*P(C_i)) (deniminator for normalization only, can be ignored)\n",
    "            for y_type in self.type_stats:\n",
    "                prob = self.calculate_prob(xs, self.type_stats[y_type])\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    guess = y_type\n",
    "                    # use to troubleshoot\n",
    "                    # print (f'max prob {max_prob}, variable prob {prob}')\n",
    "            return guess\n",
    "        else:\n",
    "            print(\"Please train the classifier first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose this function form because all the attributes are continuous; it is appopriate to represent them using a gaussian function.\n",
    "\n",
    "Now to find P(alcohol % 13 | Cultivar 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6433285835480093\n"
     ]
    }
   ],
   "source": [
    "typefilter = (y == 1)\n",
    "print(NaiveBayesClassifier.gaussian(13,x[typefilter][:,0].mean(),\\\n",
    "                                    x[typefilter][:,0].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chance of finding Alcohol % to be 13 for wines belong to cultivar 1 is 64.33% according to the gaussian PDF. I am wrong though; reference code says 23.24%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b.\n",
    "Using your normalized chemical descriptor data from the previous clustering exercise, divide your data into 3-fold training and testing groups, i.e. using 2/3 training and 1/3 testing for the three divisions. Does Naïve Baye’s perform as well as previous methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, divide the data into three groups (folds) and predict using Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_bayes(k, Xs, ys):\n",
    "    total_num = len(Xs)\n",
    "\n",
    "    kf = KFold(n_splits = k, shuffle = True)\n",
    "    train_acc_all = []\n",
    "    test_acc_all = []\n",
    "    for train_selector,test_selector in kf.split(range(total_num)):\n",
    "            # Decite training examples and testing examples for this fold\n",
    "            train_Xs = Xs[train_selector]\n",
    "            test_Xs = Xs[test_selector]\n",
    "            train_ys = ys[train_selector]\n",
    "            test_ys = ys[test_selector]\n",
    "\n",
    "            nb_classifier = NaiveBayesClassifier()\n",
    "            nb_classifier.fit(train_Xs, train_ys)\n",
    "\n",
    "            # Report result for this fold\n",
    "            train_acc = calculate_accuracy(nb_classifier, train_Xs, train_ys)\n",
    "            train_acc_all.append(train_acc)\n",
    "            test_acc = calculate_accuracy(nb_classifier, test_Xs, test_ys)\n",
    "            test_acc_all.append(test_acc)\n",
    "            print(\"Train accuracy:\", train_acc)\n",
    "            print(\"Test accuracy:\", test_acc)\n",
    "            print(\"Final results:\")\n",
    "            print(\"Training accuracy:%f+-%f\"%(np.average(train_acc_all), np.std(train_acc_all)))\n",
    "            print(\"Testing accuracy:%f+-%f\"%(np.average(test_acc_all), np.std(test_acc_all)))\n",
    "\n",
    "    print(\"Final results:\")\n",
    "    print(\"Training accuracy:%f+-%f\"%(np.average(train_acc_all), np.std(train_acc_all)))\n",
    "    print(\"Testing accuracy:%f+-%f\"%(np.average(test_acc_all), np.std(test_acc_all)))\n",
    "\n",
    "    def calculate_accuracy(model, xs ,ys):\n",
    "        y_pred = np.zeros_like(ys)\n",
    "        for idx, x in enumerate(xs):\n",
    "            y_pred[idx] = model.predict(x)\n",
    "        return np.sum(ys == y_pred) / len(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Softmax and Cross Entropy Loss\n",
    "Work on the same wine dataset. Now we use another approach to do the classification. Implement a neural network model using PyTorch with no hidden layer (This is equivalent to a linear regression plus activation function). Use softmax activation function in the last layer and use cross entropy loss as your loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(13, 3),\n",
    "        nn.Softmax(dim=1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class MLP_no_softmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_no_softmax, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(13, 3),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a\n",
    "Pass the data through the network once without backpropagation and print out the output. Observe the difference between with and without the softmax activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178, 3])\n",
      "With softmax -- \n",
      " output activation: tensor([[0.5221, 0.1499, 0.3279],\n",
      "        [0.2621, 0.2130, 0.5249],\n",
      "        [0.7140, 0.1835, 0.1025],\n",
      "        [0.5797, 0.2577, 0.1626],\n",
      "        [0.5458, 0.2704, 0.1838],\n",
      "        [0.4379, 0.1950, 0.3672],\n",
      "        [0.5360, 0.1628, 0.3012],\n",
      "        [0.3310, 0.3488, 0.3202],\n",
      "        [0.3790, 0.2319, 0.3891],\n",
      "        [0.4151, 0.4259, 0.1590]], grad_fn=<SliceBackward0>) \n",
      " sum of output activation: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], grad_fn=<SliceBackward0>)\n",
      "Without softmax -- \n",
      " output activation: tensor([[ 0.2462, -0.4256, -0.7235],\n",
      "        [ 0.0895, -0.2664, -0.3552],\n",
      "        [-0.1447, -0.1014,  0.1861],\n",
      "        [ 0.0679, -0.2058,  0.6701],\n",
      "        [-0.0116, -0.1585,  0.3070],\n",
      "        [ 0.4828, -0.7228, -0.1415],\n",
      "        [ 0.1742, -0.1797, -0.6500],\n",
      "        [-0.3165,  0.0377,  0.0614],\n",
      "        [-0.2695,  0.2295,  0.0997],\n",
      "        [-0.1448, -0.0547,  0.7246]], grad_fn=<SliceBackward0>) \n",
      " sum of output activation: tensor([-0.9030, -0.5321, -0.0601,  0.5321,  0.1369, -0.3814, -0.6554, -0.2175,\n",
      "         0.0597,  0.5250], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model1=MLP()\n",
    "model2 = MLP_no_softmax()\n",
    "\n",
    "train_in =torch.tensor(x_norm, dtype=torch.float)\n",
    "print(model1(train_in).shape)\n",
    "\n",
    "print(\"With softmax --\\n output activation:\",model1(train_in)[:10], '\\n sum of output activation:',torch.sum(model1(train_in),axis=1)[:10])\n",
    "print(\"Without softmax -- \\n output activation:\",model2(train_in)[:10],'\\n sum of output activation:',torch.sum(model2(train_in),axis=1)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b\n",
    "Divide your data into 3-fold training and testing groups, within each fold further divide your training data into 80% training and 20% validation, choose the model for the epoch with lowest validation error. Report error in terms of success rate of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_val(model, train_X, train_y, epochs, draw_curve = True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: a PyTorch model\n",
    "    train_X: np.array shape(ndata,nfeatures)\n",
    "    train_y: np.array shape(ndata)\n",
    "    epochs: int\n",
    "    draw_curve: bool\n",
    "    \"\"\"\n",
    "    ### Define your loss function, optimizer. Convert data to torch tensor ###\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    train_X = torch.tensor(train_X, dtype=torch.float)\n",
    "    train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "\n",
    "    ### Split training examples further into training and validation ###\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size = 0.2)\n",
    "\n",
    "    val_array=[]\n",
    "    lowest_val_loss = np.inf\n",
    "    weights = model.state_dict()\n",
    "    model_param = model.state_dict()\n",
    "   \n",
    "    for i in range(epochs):\n",
    "        ### Compute the loss and do backpropagation ###\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train)\n",
    "        loss = loss_func(y_pred, y_train-1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        ### compute validation loss and keep track of the lowest val loss ###\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = loss_func(val_pred, y_val - 1).detach().numpy()\n",
    "            val_array.append(val_loss)\n",
    "           \n",
    "            if val_loss < lowest_val_loss:\n",
    "                lowest_val_loss = val_loss\n",
    "                weights = model.state_dict()\n",
    "               \n",
    "     # The final number of epochs is when the minimum error in validation set occurs    \n",
    "    final_epochs = np.argmin(val_array) + 1\n",
    "    print(\"Number of epochs with lowest validation:\", final_epochs)\n",
    "    ### Recover the model weight ###\n",
    "    model.load_state_dict(weights)\n",
    "\n",
    "    if draw_curve:\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(val_array)) + 1, val_array, label = 'Validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
